use std::convert::TryFrom;
use std::ops::{Deref, DerefMut};

use http::uri::InvalidUri;
use tonic::metadata::{AsciiMetadataValue, MetadataValue};
use tonic::service::Interceptor;
use tonic::transport::channel::ClientTlsConfig;
use tonic::{service::interceptor::InterceptedService, Status};
use tonic::{transport::Channel, Request};

use super::inference;
use super::inference::grpc_inference_service_client::GrpcInferenceServiceClient;

#[derive(Clone)]
pub struct AuthInterceptor {
    token: Option<AsciiMetadataValue>,
}

impl Interceptor for AuthInterceptor {
    fn call(&mut self, mut request: tonic::Request<()>) -> Result<tonic::Request<()>, Status> {
        if let Some(token) = self.token.as_ref() {
            request
                .metadata_mut()
                .insert("authorization", token.clone());
        }

        Ok(request)
    }
}

impl AuthInterceptor {
    fn create(access_token: Option<&str>) -> Result<Self, Error> {
        if let Some(access_token) = access_token {
            let fmt_token: String = format!("Bearer {}", access_token);
            let token = Some(MetadataValue::try_from(fmt_token)?);
            Ok(AuthInterceptor { token })
        } else {
            Ok(AuthInterceptor { token: None })
        }
    }
}

#[derive(Debug, Clone)]
pub struct Client {
    /// Raw grpc client interfaces automatically generated by tonic
    ///
    /// Should not necessary to use this interface directly in most cases
    pub inner: GrpcInferenceServiceClient<InterceptedService<Channel, AuthInterceptor>>,
}

#[derive(thiserror::Error, Debug)]
pub enum Error {
    #[error("a gRPC transport error has occurred: {0}")]
    TransportError(#[from] tonic::transport::Error),
    #[error("the client was provided and invalid URI: {0}")]
    InvalidUri(#[from] InvalidUri),
    #[error("invalid access token")]
    InvalidAccessToken(#[from] tonic::metadata::errors::InvalidMetadataValue),
    #[error("grpc call returned error status")]
    ResponseError(#[from] Status),
}

macro_rules! wrap_grpc_method {
    ($doc:literal, $name:ident, $req_type:ty, $resp_type:ty) => {
        #[doc=$doc]
        pub async fn $name(&self, req: $req_type) -> Result<$resp_type, Error> {
            let response = self.inner.clone().$name(tonic::Request::new(req)).await?;
            Ok(response.into_inner())
        }
    };
}

macro_rules! wrap_grpc_method_no_args {
    ($doc:literal, $name:ident, $req_type:ty, $resp_type:ty) => {
        #[doc=$doc]
        pub async fn $name(&self) -> Result<$resp_type, Error> {
            let req: $req_type = Default::default();
            let response = self.inner.clone().$name(tonic::Request::new(req)).await?;
            Ok(response.into_inner())
        }
    };
}

impl Client {
    /// Create a new triton client for the given url.
    pub async fn new(url: String, access_token: Option<String>) -> Result<Self, Error> {
        let mut channel = Channel::from_shared(url)?;

        if access_token.is_some() {
            channel = channel.tls_config(ClientTlsConfig::new())?;
        }

        let channel = channel.connect().await?;

        let client = GrpcInferenceServiceClient::with_interceptor(
            channel,
            AuthInterceptor::create(access_token.as_deref())?,
        );

        Ok(Client { inner: client })
    }

    wrap_grpc_method_no_args!(
        "Check liveness of the inference server.",
        server_live,
        inference::ServerLiveRequest,
        inference::ServerLiveResponse
    );

    wrap_grpc_method_no_args!(
        "Check readiness of the inference server.",
        server_ready,
        inference::ServerReadyRequest,
        inference::ServerReadyResponse
    );
    wrap_grpc_method!(
        "Check readiness of a model in the inference server.",
        model_ready,
        inference::ModelReadyRequest,
        inference::ModelReadyResponse
    );
    wrap_grpc_method_no_args!(
        "Get server metadata.",
        server_metadata,
        inference::ServerMetadataRequest,
        inference::ServerMetadataResponse
    );
    wrap_grpc_method!(
        "Get model metadata.",
        model_metadata,
        inference::ModelMetadataRequest,
        inference::ModelMetadataResponse
    );
    wrap_grpc_method!(
        "Perform inference using a specific model.",
        model_infer,
        inference::ModelInferRequest,
        inference::ModelInferResponse
    );
    wrap_grpc_method!(
        "Get model configuration.",
        model_config,
        inference::ModelConfigRequest,
        inference::ModelConfigResponse
    );
    wrap_grpc_method!(
        "Get the cumulative inference statistics for a model.",
        model_statistics,
        inference::ModelStatisticsRequest,
        inference::ModelStatisticsResponse
    );
    wrap_grpc_method!(
        "Get the index of model repository contents.",
        repository_index,
        inference::RepositoryIndexRequest,
        inference::RepositoryIndexResponse
    );
    wrap_grpc_method!(
        "Load or reload a model from a repository.",
        repository_model_load,
        inference::RepositoryModelLoadRequest,
        inference::RepositoryModelLoadResponse
    );
    wrap_grpc_method!(
        "Unload a model.",
        repository_model_unload,
        inference::RepositoryModelUnloadRequest,
        inference::RepositoryModelUnloadResponse
    );
}
